{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8j0hsOoM03e"
   },
   "source": [
    "# Task 1: Acquire, pre-process and analyze the data\n",
    "## Acquiring both datasets:\n",
    "Dataset 1: [Search Trends](https://github.com/google-research/open-covid-19-data/blob/master/data/exports/search_trends_symptoms_dataset/README.mdhttps://)\n",
    "\n",
    "Dataset 2: [COVID hospitalization cases](https://github.com/google-research/open-covid-19-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eW-miWaPR0Yt"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Dza6al16Mj74"
   },
   "outputs": [],
   "source": [
    "# the week of 08/24/2020 for the data collection\n",
    "# Load into pandas dataframes\n",
    "st_df = pd.read_csv('2020_US_weekly_symptoms_dataset.csv', low_memory=False)\n",
    "hp_df = pd.read_csv('aggregated_cc_by.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elRSVGYhZi7L"
   },
   "source": [
    "## Preprocess the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Weeks range: 2020-03-09 to 2020-09-21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JZ2ApGzrZgcZ"
   },
   "outputs": [],
   "source": [
    "# Search trends dataset Part I\n",
    "\n",
    "#TODO: Preprocessing, remove all symptoms that have all zero entries (clean COLUMN)\n",
    "st_df = st_df.dropna(how='all', axis=1)\n",
    "\n",
    "#Remove all rows not in the date of the week chosen (clean ROW)\n",
    "st_df = st_df[st_df['date'] >= '2020-03-04']\n",
    "\n",
    "nameList = list(st_df['sub_region_1']) #extract the region names from st_df database\n",
    "nameList = list(dict.fromkeys(nameList))  #remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hospitalization dataset Part I\n",
    "\n",
    "#TODO: Preprocessing\n",
    "\n",
    "#keep the hospitalization features and delete the rest  (clean COLUMN)\n",
    "hp_df = hp_df[['open_covid_region_code','region_name','date', 'hospitalized_new']]\n",
    "\n",
    "#select the regions that match the Search trends dataset (clean ROW)\n",
    "hp_df= hp_df[hp_df.region_name.isin(nameList)]\n",
    "\n",
    "#select the regions that have the valid date range (clean ROW)\n",
    "hp_df = hp_df[(hp_df['date'] >= '2020-03-09') & (hp_df['date'] <= '2020-09-27')]\n",
    "\n",
    "hp_df.reset_index(inplace = True) \n",
    "# print(hp_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hospitalization dataset Part II\n",
    "# Here we want to group dates in the same week together as one date (the weekdate)\n",
    "hp_df1 = hp_df\n",
    "weekdate = '2020-03-09'\n",
    "\n",
    "#This loop will update all the dates row by row\n",
    "for i, n in hp_df1.iterrows():\n",
    "    if (i%7 == 0):\n",
    "        weekdate = n['date']  #first date of the week\n",
    "    else:\n",
    "        hp_df1.at[i,'date'] = weekdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      region_name        date  hospitalized_new  index open_covid_region_code\n",
      "3          Hawaii  2020-03-09             802.0  87097                  US-HI\n",
      "4           Idaho  2020-03-09            1811.0  86678                  US-ID\n",
      "5           Maine  2020-03-09             445.0  84953                  US-ME\n",
      "6         Montana  2020-03-09             687.0  83701                  US-MT\n",
      "7        Nebraska  2020-03-09            2279.0  83071                  US-NE\n",
      "8   New Hampshire  2020-03-09             736.0  82860                  US-NH\n",
      "9      New Mexico  2020-03-09            3435.0  82413                  US-NM\n",
      "10   North Dakota  2020-03-09             815.0  83280                  US-ND\n",
      "11   Rhode Island  2020-03-09            2725.0  80738                  US-RI\n",
      "12   South Dakota  2020-03-09            1473.0  80311                  US-SD\n",
      "15        Wyoming  2020-03-09             262.0  78166                  US-WY\n"
     ]
    }
   ],
   "source": [
    "#sum up the hospitalized_vew for weekly\n",
    "# we are only using this hp_df2 to rid regions that have insignificant hospitalized data, such as 0 for total hospitalization\n",
    "def cleanRegions(df):\n",
    "    hp_df2 = df\n",
    "    f = dict.fromkeys(hp_df2.columns.difference(['region_name']), 'first')\n",
    "    f['hospitalized_new'] = sum\n",
    "    hp_df2 = hp_df2.groupby('region_name', as_index=False).agg(f)\n",
    "    hp_df2 = hp_df2[hp_df2.hospitalized_new != 0]\n",
    "    print(hp_df2.to_string())\n",
    "    tmplist = list(hp_df2['region_name']) \n",
    "    tmplist = list(dict.fromkeys(tmplist))  \n",
    "    return(tmplist)\n",
    "\n",
    "#this nameList will be a new regions list that removes region with total of 0 hospitalization value for all its dates\n",
    "nameList2 = cleanRegions(hp_df1)\n",
    "\n",
    "#filter hp_df1 based on the nameList2 (clean ROW)\n",
    "hp_df2= hp_df1[hp_df1.region_name.isin(nameList2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hospitalization dataset Part III\n",
    "\n",
    "# merge 7 week rows into 1 and sum up the hospitalized_new data\n",
    "hp_df3 = hp_df2.groupby(['region_name','date'])['hospitalized_new'].apply(sum).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search trends dataset Part II\n",
    "\n",
    "# Drop unnecessary columns (open_covid region_code, country_region_code, country_region) (clean COLUMN)\n",
    "st_df1 = st_df.drop(st_df.columns[[0, 1, 2]], axis=1)\n",
    "\n",
    "# Filter st_df based on nameList2 (clean ROW)\n",
    "st_df1= st_df1[st_df.sub_region_1.isin(nameList2)]\n",
    "# print(st_df1)\n",
    "# print(st_df1.shape)\n",
    "\n",
    "#Filter columns so that every column have at least sp_num% of non-zero entries  (clean COLUMN)\n",
    "sp_num = 0.24  #optimized ratio without tremendous loss of dataset\n",
    "st_df2 = st_df1.dropna(thresh=sp_num*len(st_df), axis=1)\n",
    "# print(\"after.........\" )\n",
    "# print(st_df2)\n",
    "# print(st_df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sub_region_1 sub_region_1_code        date  symptom:Adrenal crisis  \\\n",
      "123       Hawaii             US-HI  2020-03-09                     NaN   \n",
      "124       Hawaii             US-HI  2020-03-16                     NaN   \n",
      "125       Hawaii             US-HI  2020-03-23                     NaN   \n",
      "126       Hawaii             US-HI  2020-03-30                     NaN   \n",
      "127       Hawaii             US-HI  2020-04-06                     NaN   \n",
      "..           ...               ...         ...                     ...   \n",
      "603      Wyoming             US-WY  2020-08-24                    4.42   \n",
      "604      Wyoming             US-WY  2020-08-31                    5.03   \n",
      "605      Wyoming             US-WY  2020-09-07                    3.36   \n",
      "606      Wyoming             US-WY  2020-09-14                    4.15   \n",
      "607      Wyoming             US-WY  2020-09-21                    5.64   \n",
      "\n",
      "     symptom:Allergic conjunctivitis  symptom:Angular cheilitis  \\\n",
      "123                              NaN                      32.41   \n",
      "124                              NaN                      28.68   \n",
      "125                              NaN                      31.62   \n",
      "126                              NaN                      39.39   \n",
      "127                              NaN                      36.26   \n",
      "..                               ...                        ...   \n",
      "603                             3.42                        NaN   \n",
      "604                              NaN                        NaN   \n",
      "605                             2.20                       2.25   \n",
      "606                              NaN                        NaN   \n",
      "607                              NaN                        NaN   \n",
      "\n",
      "     symptom:Aphonia  symptom:Asphyxia  symptom:Atheroma  \\\n",
      "123            64.27               NaN               NaN   \n",
      "124            50.26               NaN               NaN   \n",
      "125            36.37               NaN               NaN   \n",
      "126            28.45               NaN               NaN   \n",
      "127            23.07               NaN               NaN   \n",
      "..               ...               ...               ...   \n",
      "603              NaN              3.75              2.93   \n",
      "604              NaN              5.08              3.46   \n",
      "605              NaN              4.02               NaN   \n",
      "606              NaN              4.04              3.71   \n",
      "607              NaN              2.90              5.53   \n",
      "\n",
      "     symptom:Auditory hallucination  ...  symptom:Rumination  \\\n",
      "123                             NaN  ...                 NaN   \n",
      "124                             NaN  ...                 NaN   \n",
      "125                             NaN  ...                 NaN   \n",
      "126                             NaN  ...                 NaN   \n",
      "127                             NaN  ...                 NaN   \n",
      "..                              ...  ...                 ...   \n",
      "603                            4.53  ...                 NaN   \n",
      "604                             NaN  ...                2.85   \n",
      "605                            2.42  ...                2.86   \n",
      "606                            3.61  ...                3.88   \n",
      "607                            2.74  ...                2.24   \n",
      "\n",
      "     symptom:Shallow breathing  symptom:Stridor  symptom:Subdural hematoma  \\\n",
      "123                      49.58              NaN                        NaN   \n",
      "124                      83.77              NaN                        NaN   \n",
      "125                      73.87              NaN                        NaN   \n",
      "126                      60.19              NaN                        NaN   \n",
      "127                      44.50              NaN                        NaN   \n",
      "..                         ...              ...                        ...   \n",
      "603                        NaN              NaN                       2.82   \n",
      "604                       2.90              NaN                       2.85   \n",
      "605                       2.20             2.48                       5.23   \n",
      "606                        NaN             2.57                       3.22   \n",
      "607                        NaN             2.74                       4.32   \n",
      "\n",
      "     symptom:Tenderness  symptom:Trichoptilosis  symptom:Urinary urgency  \\\n",
      "123                 NaN                     NaN                      NaN   \n",
      "124                 NaN                     NaN                      NaN   \n",
      "125                 NaN                     NaN                      NaN   \n",
      "126                 NaN                     NaN                      NaN   \n",
      "127                 NaN                     NaN                      NaN   \n",
      "..                  ...                     ...                      ...   \n",
      "603                3.98                    2.26                      NaN   \n",
      "604                3.07                    2.57                      NaN   \n",
      "605                4.29                     NaN                      NaN   \n",
      "606                3.93                    3.06                     2.68   \n",
      "607                3.28                    3.17                      NaN   \n",
      "\n",
      "     symptom:Ventricular fibrillation  symptom:Viral pneumonia  \\\n",
      "123                               NaN                   100.00   \n",
      "124                               NaN                    88.60   \n",
      "125                               NaN                    70.51   \n",
      "126                               NaN                    46.78   \n",
      "127                               NaN                    28.57   \n",
      "..                                ...                      ...   \n",
      "603                              2.48                      NaN   \n",
      "604                               NaN                      NaN   \n",
      "605                               NaN                      NaN   \n",
      "606                               NaN                      NaN   \n",
      "607                              3.23                      NaN   \n",
      "\n",
      "     hospitalized_new  \n",
      "123               0.0  \n",
      "124               0.0  \n",
      "125              12.0  \n",
      "126               7.0  \n",
      "127              25.0  \n",
      "..                ...  \n",
      "603               8.0  \n",
      "604               4.0  \n",
      "605               9.0  \n",
      "606              15.0  \n",
      "607              19.0  \n",
      "\n",
      "[319 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "hpData = hp_df3[\"hospitalized_new\"]\n",
    "hpData = pd.Series(hpData)\n",
    "\n",
    "st_df2['hospitalized_new'] = hpData.values # Merging the data_set\n",
    "print(st_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert merged dataset into a numpy array\n",
    "myarray = pd.DataFrame(st_df2).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Visualize and Cluster the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the evolution of the search frequency of popular symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values by 0s\n",
    "myarray[pd.isnull(myarray)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = myarray[:,0]\n",
    "time = myarray[:,2]\n",
    "features = myarray[:,3:-1].astype(float)\n",
    "label = myarray[:,-1].astype(int)\n",
    "\n",
    "nData, nFeat = features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of symptom names\n",
    "nameSymptoms = np.array([])\n",
    "for name in st_df2.columns.values[3:-1]:\n",
    "    nameSymptoms = np.append(nameSymptoms, name.lstrip('symptom:'))\n",
    "\n",
    "# print(nameSymptoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most popular searches (rank by # of nonzero instances)\n",
    "mostPop = 5\n",
    "\n",
    "arrInst = np.count_nonzero(features, axis=0)\n",
    "# Get indices of most popular searches\n",
    "topSearchInd = np.sort(np.argpartition(arrInst, -mostPop)[-mostPop:])\n",
    "# print(topSearchInd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of symptom names of most popular searches\n",
    "namePopSymptoms = nameSymptoms[topSearchInd]\n",
    "# print(namePopSymptoms)\n",
    "\n",
    "# Get array of features of only the most popular searches\n",
    "popFeatures = features[:,topSearchInd]\n",
    "# print(popFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape search data 2D array into 3D [region, time, feature]\n",
    "\n",
    "uniqueRegions, ctRegions = np.unique(regions, return_counts=True)\n",
    "# print(dict(zip(uniqueRegions, ctRegions)))\n",
    "\n",
    "uniqueTime, ctTime = np.unique(time, return_counts=True)\n",
    "# print(dict(zip(uniqueTime, ctTime)))\n",
    "\n",
    "nRegions = ctTime[0]\n",
    "nTime = ctRegions[0]\n",
    "\n",
    "sdArr = popFeatures.reshape(-1, nTime, mostPop)\n",
    "# print(sdArr.shape)\n",
    "# print(sdArr[0,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot heatmap\n",
    "# for i in range(mostPop):\n",
    "#     fig, ax = plt.subplots(figsize=(20, 10))\n",
    "#     im = ax.imshow(sdArr[:,:,i], cmap='magma', vmin=0, vmax=100)\n",
    "#     plt.colorbar(im)\n",
    "\n",
    "#     ax.set_xticks(np.arange(nTime))\n",
    "#     ax.set_yticks(np.arange(nRegions))\n",
    "\n",
    "#     ax.set_xticklabels(uniqueTime)\n",
    "#     ax.set_yticklabels(uniqueRegions)\n",
    "\n",
    "#     plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "\n",
    "#     for j in range(nTime):\n",
    "#         for k in range(nRegions):\n",
    "#             text = ax.text(j, k, sdArr[k, j, i], ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "#     ax.set_title('Evolution of ' + namePopSymptoms[i])\n",
    "    \n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing\n",
    "dataHeatmap = np.swapaxes(sdArr, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399ecda16e3c4ae8aa3af30bd3c78f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='TimeSlider', max=28), Output()), _dom_classes=('widget-iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.timeSlider(TimeSlider=0)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "def timeSlider(TimeSlider=0):\n",
    "#     print('Week of ' + str(uniqueTime[TimeSlider]))\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    ax.set_xticks(np.arange(nRegions))\n",
    "    ax.set_yticks(np.arange(mostPop))\n",
    "\n",
    "    ax.set_xticklabels(uniqueRegions)\n",
    "    ax.set_yticklabels(namePopSymptoms)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "    \n",
    "    ax.set_title('Evolution of symptom search trends -- Week of ' + str(uniqueTime[TimeSlider]))\n",
    "    im = ax.imshow(dataHeatmap[:,TimeSlider,:], cmap='magma', vmin=0, vmax=100)\n",
    "    plt.colorbar(im)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "        \n",
    "widgets.interact(timeSlider, TimeSlider=(0, nTime-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simple graph -- each region\n",
    "\n",
    "# for i in range(mostPop):\n",
    "#     fig = plt.figure(figsize=(20,10))\n",
    "#     plt.xticks(np.arange(nTime), uniqueTime, rotation=45, ha='right', rotation_mode='anchor')\n",
    "#     plt.yticks(np.arange(0, 100, step=5))\n",
    "#     plt.title('Evolution of ' + namePopSymptoms[i])\n",
    "\n",
    "#     for j in range(nRegions):\n",
    "#         plt.plot(sdArr[j,:,i], label=uniqueRegions[j], marker='o', linestyle='dashed', linewidth=1, markersize=5)\n",
    "\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simple graph -- avg of all regions\n",
    "\n",
    "# for i in range(mostPop):\n",
    "#     fig = plt.figure(figsize=(20,10))\n",
    "#     plt.xticks(np.arange(nTime), uniqueTime, rotation=45, ha='right', rotation_mode='anchor')\n",
    "#     plt.yticks(np.arange(0, 100, step=5))\n",
    "#     plt.title('Evolution of ' + namePopSymptoms[i])\n",
    "    \n",
    "#     avgSDArr = np.average(sdArr[:,:,i], axis=0)\n",
    "#     plt.plot(avgSDArr, label='Average Across All Regions', marker='o', linestyle='dashed', linewidth=5, markersize=10)\n",
    "\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simple graph -- each region + average of all regions combined\n",
    "\n",
    "# for i in range(mostPop):\n",
    "#     fig = plt.figure(figsize=(20,10))\n",
    "#     plt.xticks(np.arange(nTime), uniqueTime, rotation=45, ha='right', rotation_mode='anchor')\n",
    "#     plt.yticks(np.arange(0, 100, step=5))\n",
    "#     plt.title('Evolution of ' + namePopSymptoms[i])\n",
    "\n",
    "#     for j in range(nRegions):\n",
    "#         plt.plot(sdArr[j,:,i], label=uniqueRegions[j], marker='o', linestyle='dashed', linewidth=1, markersize=5)\n",
    "        \n",
    "#     avgSDArr = np.average(sdArr[:,:,i], axis=0)\n",
    "#     plt.plot(avgSDArr, label='Average Across All Regions', marker='o', linestyle='dashed', linewidth=5, markersize=10)\n",
    "\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bar graph -- stacked for all regions\n",
    "\n",
    "# for i in range(mostPop):\n",
    "#     fig = plt.figure(figsize=(20,10))\n",
    "#     plt.xticks(np.arange(nTime), uniqueTime, rotation=45, ha='right', rotation_mode='anchor')\n",
    "#     plt.yticks(np.arange(0, mostPop*100, step=mostPop*5))\n",
    "#     plt.title('Evolution of ' + namePopSymptoms[i])\n",
    "    \n",
    "#     tmpSdArr = np.zeros_like(sdArr[0,:,i])\n",
    "#     for j in range(nRegions):\n",
    "#         plt.bar(np.arange(nTime), sdArr[j,:,i], bottom=tmpSdArr, label=uniqueRegions[j])\n",
    "#         tmpSdArr = np.add(tmpSdArr, sdArr[j,:,i])\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import RadioButtons, Slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PCA to reduce data dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the features\n",
    "pca = PCA()\n",
    "pca.fit(features)\n",
    "pcaEVR = pca.explained_variance_ratio_\n",
    "totalPC = len(pcaEVR)\n",
    "cumulativeVar = 100*np.cumsum(pcaEVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshCVar = 95\n",
    "\n",
    "nPC = np.where(cumulativeVar > threshCVar)[0][0]\n",
    "cVar = cumulativeVar[nPC]\n",
    "nPC += 1\n",
    "\n",
    "# print(cVar, nPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cumulative variance vs # of principal components to choose #PCs\n",
    "fig = plt.figure()\n",
    "plt.plot(np.linspace(1, totalPC, totalPC), threshCVar*np.ones((totalPC,)), 'c--')\n",
    "plt.plot(np.linspace(1, totalPC, totalPC), cumulativeVar)\n",
    "plt.plot(nPC, cVar, 'o')\n",
    "\n",
    "plt.text(nPC-3, cVar-4, 'Optimal #PCs: ' + str(nPC))\n",
    "plt.text(totalPC, threshCVar, 'Cumulative Variance Threshold:' + str(threshCVar) + '%', horizontalalignment='right')\n",
    "\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"% Variance Explained\")\n",
    "plt.title(\"Cumulative Variance Explained vs Number of Principal Components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have chosen the #PCs -> now reduce the search trends dataset to this dimensionality\n",
    "pcaRed = PCA(n_components=nPC)\n",
    "pcaRed.fit(features)\n",
    "reducedFeat = pcaRed.transform(features)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(reducedFeat[:,0], reducedFeat[:,1])\n",
    "plt.xlabel(\"PC #1\")\n",
    "plt.ylabel(\"PC #2\")\n",
    "plt.title(\"Search Trends Dataset Reduced to \" + str(nPC) + \"D, Visualizing in 2D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using k-means clustering to evaluate groups in search trends data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nClusters = 6    # To be tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering on PCA-reduced data\n",
    "kmeansRed = KMeans(n_clusters=nClusters, random_state=0)\n",
    "kmeansRed.fit(reducedFeat)\n",
    "y_PredRed = kmeansRed.predict(reducedFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering on original (non-reduced) data\n",
    "kmeansOri = KMeans(n_clusters=nClusters, random_state=0)\n",
    "kmeansOri.fit(features)\n",
    "y_PredOri = kmeansOri.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clustering results for both reduced and non-reduced data\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.scatter(reducedFeat[:,0], reducedFeat[:,1], c=y_PredRed, cmap=plt.cm.get_cmap('viridis', nClusters))\n",
    "plt.colorbar(ticks=range(nClusters))\n",
    "plt.xlabel(\"PC #1\")\n",
    "plt.ylabel(\"PC #2\")\n",
    "plt.title(\"Cluster Labels for \" + str(nPC) + \"D K-Means with \" + str(nClusters) + \" Clusters\")\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.scatter(reducedFeat[:,0], reducedFeat[:,1], c=y_PredOri, cmap=plt.cm.get_cmap('viridis', nClusters))\n",
    "plt.colorbar(ticks=range(nClusters))\n",
    "plt.xlabel(\"PC #1\")\n",
    "plt.ylabel(\"PC #2\")\n",
    "plt.title(\"Cluster Labels for \" + str(totalPC) + \"D K-Means (Unreduced Data) with \" + str(nClusters) + \" Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from  sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace NaN values to be 0's\n",
    "myarray[pd.isnull(myarray)] = 0\n",
    "print(myarray.shape)\n",
    "\n",
    "#data_r is sorted based on regions\n",
    "data_r = myarray\n",
    "#data_t is sorted based on time\n",
    "data_t = np.array(sorted(myarray,key = lambda x: x[2]))\n",
    "\n",
    "#split the region-based data to be regions, features, and label\n",
    "regions_r = data_r[:,0]\n",
    "features_r = data_r[:,3:-1].astype(float)\n",
    "label_r = data_r[:,-1].astype(int)\n",
    "#split the time-based data to be time, features, and label\n",
    "time_t = data_t[:,2]\n",
    "features_t = data_t[:,3:-1].astype(float)\n",
    "label_t = data_t[:,-1].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region-based cross-validation (5-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the region-based data into 5 folds based on regions\n",
    "from sklearn.model_selection import GroupKFold\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "#specify the group indices by regions\n",
    "groups = regions_r\n",
    "for i in range(groups.shape[0]):\n",
    "    if groups[i] == 'Hawaii' or groups[i] ==  'Idaho':\n",
    "        groups[i] = 1\n",
    "    elif groups[i] == 'Maine' or groups[i] ==  'Montana':\n",
    "        groups[i] = 2\n",
    "    elif groups[i] ==  'North Dakota' or groups[i] ==  'Nebraska':\n",
    "        groups[i] = 3\n",
    "    elif groups[i] ==  'New Hampshire' or groups[i] ==  'New Mexico':\n",
    "        groups[i] = 4\n",
    "    else:\n",
    "        groups[i] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (region-based validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],w
   "source": [
    "#plot cross-validated MSE error of KNN with respect to different values of k\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "k_range_r = range(1, 16)\n",
    "k_scores_r = []\n",
    "for k in k_range_r:\n",
    "    knn_r = neighbors.KNeighborsRegressor(n_neighbors=k)\n",
    "    loss = abs(cross_val_score(knn_r, features_r,label_r, cv=gkf.split(features_r,label_r, groups), scoring='neg_mean_squared_error'))\n",
    "    k_scores_r.append(loss.mean())\n",
    "plt.plot(k_range_r, k_scores_r)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=14 leads to the least MSE, whcih can represent performance of KNN\n",
    "knn_r_error = k_scores_r[13]\n",
    "print(knn_r_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree (region-based validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate cross-validated MSE of decision tree\n",
    "dt_r = tree.DecisionTreeRegressor(random_state=1)\n",
    "loss = abs(cross_val_score(dt_r, features_r, label_r, cv = gkf.split(features_r,label_r, groups), scoring='neg_mean_squared_error'))\n",
    "dt_r_error = loss.mean()\n",
    "print(dt_r_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-based validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test sets based on the timepoint '2020-08-10'\n",
    "#data before '2020-08-10' are in train set and the rest are in test set\n",
    "#time_t[242] is the first '2020-08-10' which is the splitting point\n",
    "features_t_train = features_t[0:241]\n",
    "label_t_train = label_t[0:241]\n",
    "features_t_test = features_t[241:320]\n",
    "label_t_test = label_t[241:320]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (time-based validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot validated MSE error of KNN with respect to different values of k\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "k_range_t = range(1, 16)\n",
    "k_scores_t= []\n",
    "for k in k_range_t:\n",
    "    knn_t = neighbors.KNeighborsRegressor(n_neighbors = k)\n",
    "    knn_t.fit(features_t_train, label_t_train)\n",
    "    label_t_pred_knn = knn_t.predict(features_t_test)\n",
    "    loss = mean_squared_error(label_t_test, label_t_pred_knn)\n",
    "    k_scores_t.append(loss)\n",
    "plt.plot(k_range_t, k_scores_t)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Validated MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=13 leads to the least MSE, whcih can represent performance of KNN\n",
    "knn_t_error = k_scores_t[12]\n",
    "print(knn_t_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree (time-based validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate validated MSE of decision tree\n",
    "dt_t = tree.DecisionTreeRegressor(random_state=1)\n",
    "dt_t.fit(features_t_train, label_t_train)\n",
    "label_t_pred_dt = dt_t.predict(features_t_test)\n",
    "dt_t_error = mean_squared_error(label_t_test, label_t_pred_dt)\n",
    "print(dt_t_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional part: learn seperate model for each region with time-based validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (regional models with time-based validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot average validated KNN MSE error of all regions with respect to different values of k\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "k_range_regional = range(1, 16)\n",
    "k_scores_regional= []\n",
    "\n",
    "for k in k_range_regional:\n",
    "    regional_MSE_knn = []\n",
    "    for i in range(0,groups.shape[0],29):\n",
    "        #each iteration deal with data from one city\n",
    "        regional_features = features_r[i:i+29]\n",
    "        regional_label = label_r[i:i+29]\n",
    "        #split the data into train and test sets based on the timepoint 2020-08-10\n",
    "        #data before 2020-08-10 are in train set and the rest are in test set\n",
    "        #for data of each region, the 24th. datum is of '2020-08-10' which is the splitting point\n",
    "        regional_features_train = regional_features[0:23]\n",
    "        regional_label_train = regional_label[0:23]\n",
    "        regional_features_test = regional_features[23:29]\n",
    "        regional_label_test = regional_label[23:29]\n",
    "        #calculate the KNN MSE of the city of this iteration\n",
    "        knn = neighbors.KNeighborsRegressor(n_neighbors = k)\n",
    "        knn.fit(regional_features_train, regional_label_train)\n",
    "        regional_label_pred_knn = knn.predict(regional_features_test)\n",
    "        loss_knn = mean_squared_error(regional_label_test, regional_label_pred_knn)\n",
    "        regional_MSE_knn.append(loss_knn)\n",
    "    #append the average validated MSE of the current k\n",
    "    regional_MSE_knn = np.array(regional_MSE_knn)\n",
    "    k_scores_regional.append(regional_MSE_knn.mean())\n",
    "\n",
    "plt.plot(k_range_regional, k_scores_regional)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Average Validated MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=13 leads to the least average MSE, whcih can represent performance of KNN\n",
    "knn_regional_error = k_scores_regional[12]\n",
    "print(knn_regional_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree (regional models with time-based validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the average validated decision tree MSE error of all regions\n",
    "regional_MSE_dt = []\n",
    "for i in range(0,groups.shape[0],29):\n",
    "    #each iteration deal with data from one city\n",
    "    regional_features = features_r[i:i+29]\n",
    "    regional_label = label_r[i:i+29]\n",
    "    #split the data into train and test sets based on the timepoint 2020-08-10\n",
    "    #data before 2020-08-10 are in train set and the rest are in test set\n",
    "    #for data of each region, the 24th. datum is of '2020-08-10' which is the splitting point\n",
    "    regional_features_train = regional_features[0:23]\n",
    "    regional_label_train = regional_label[0:23]\n",
    "    regional_features_test = regional_features[23:29]\n",
    "    regional_label_test = regional_label[23:29]\n",
    "    #calculate the decision tree MSE of the city of this iteration\n",
    "    dt = tree.DecisionTreeRegressor(random_state=1)\n",
    "    dt.fit(regional_features_train, regional_label_train)\n",
    "    regional_label_pred_dt = dt.predict(regional_features_test)\n",
    "    loss_dt = mean_squared_error(regional_label_test, regional_label_pred_dt)\n",
    "    regional_MSE_dt.append(loss_dt)\n",
    "    \n",
    "regional_MSE_dt = np.array(regional_MSE_dt)\n",
    "dt_regional_error = regional_MSE_dt.mean()\n",
    "print(dt_regional_error)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN0fg+0w8Z9nRsJs3Ucih0p",
   "include_colab_link": true,
   "name": "COMP551_Project_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
