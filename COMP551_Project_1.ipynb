{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8j0hsOoM03e"
   },
   "source": [
    "# Task 1: Acquire, pre-process and analyze the data\n",
    "## Acquiring both datasets:\n",
    "Dataset 1: [Search Trends](https://github.com/google-research/open-covid-19-data/blob/master/data/exports/search_trends_symptoms_dataset/README.mdhttps://)\n",
    "\n",
    "Dataset 2: [COVID hospitalization cases](https://github.com/google-research/open-covid-19-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eW-miWaPR0Yt"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dza6al16Mj74"
   },
   "outputs": [],
   "source": [
    "# the week of 08/24/2020 for the data collection\n",
    "# Load into pandas dataframes\n",
    "st_df = pd.read_csv('2020_US_weekly_symptoms_dataset.csv', low_memory=False)\n",
    "hp_df = pd.read_csv('aggregated_cc_by.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elRSVGYhZi7L"
   },
   "source": [
    "## Preprocess the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Weeks range: 2020-03-09 to 2020-09-21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZ2ApGzrZgcZ"
   },
   "outputs": [],
   "source": [
    "# Search trends dataset Part I\n",
    "\n",
    "#TODO: Preprocessing, remove all symptoms that have all zero entries (clean COLUMN)\n",
    "st_df = st_df.dropna(how='all', axis=1)\n",
    "\n",
    "#Remove all rows not in the date of the week chosen (clean ROW)\n",
    "st_df = st_df[st_df['date'] >= '2020-03-04']\n",
    "\n",
    "nameList = list(st_df['sub_region_1']) #extract the region names from st_df database\n",
    "nameList = list(dict.fromkeys(nameList))  #remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hospitalization dataset Part I\n",
    "\n",
    "#TODO: Preprocessing\n",
    "\n",
    "#keep the hospitalization features and delete the rest  (clean COLUMN)\n",
    "hp_df = hp_df[['open_covid_region_code','region_name','date', 'hospitalized_new']]\n",
    "\n",
    "#select the regions that match the Search trends dataset (clean ROW)\n",
    "hp_df= hp_df[hp_df.region_name.isin(nameList)]\n",
    "\n",
    "#select the regions that have the valid date range (clean ROW)\n",
    "hp_df = hp_df[(hp_df['date'] >= '2020-03-09') & (hp_df['date'] <= '2020-09-27')]\n",
    "\n",
    "hp_df.reset_index(inplace = True) \n",
    "# print(hp_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hospitalization dataset Part II\n",
    "# Here we want to group dates in the same week together as one date (the weekdate)\n",
    "hp_df1 = hp_df\n",
    "weekdate = '2020-03-09'\n",
    "\n",
    "#This loop will update all the dates row by row\n",
    "for i, n in hp_df1.iterrows():\n",
    "    if (i%7 == 0):\n",
    "        weekdate = n['date']  #first date of the week\n",
    "    else:\n",
    "        hp_df1.at[i,'date'] = weekdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum up the hospitalized_vew for weekly\n",
    "# we are only using this hp_df2 to rid regions that have insignificant hospitalized data, such as 0 for total hospitalization\n",
    "def cleanRegions(df):\n",
    "    hp_df2 = df\n",
    "    f = dict.fromkeys(hp_df2.columns.difference(['region_name']), 'first')\n",
    "    f['hospitalized_new'] = sum\n",
    "    hp_df2 = hp_df2.groupby('region_name', as_index=False).agg(f)\n",
    "    hp_df2 = hp_df2[hp_df2.hospitalized_new != 0]\n",
    "    print(hp_df2.to_string())\n",
    "    tmplist = list(hp_df2['region_name']) \n",
    "    tmplist = list(dict.fromkeys(tmplist))  \n",
    "    return(tmplist)\n",
    "\n",
    "#this nameList will be a new regions list that removes region with total of 0 hospitalization value for all its dates\n",
    "nameList2 = cleanRegions(hp_df1)\n",
    "\n",
    "#filter hp_df1 based on the nameList2 (clean ROW)\n",
    "hp_df2= hp_df1[hp_df1.region_name.isin(nameList2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hospitalization dataset Part III\n",
    "\n",
    "# merge 7 week rows into 1 and sum up the hospitalized_new data\n",
    "hp_df3 = hp_df2.groupby(['region_name','date'])['hospitalized_new'].apply(sum).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search trends dataset Part II\n",
    "\n",
    "# Drop unnecessary columns (open_covid region_code, country_region_code, country_region) (clean COLUMN)\n",
    "st_df1 = st_df.drop(st_df.columns[[0, 1, 2]], axis=1)\n",
    "\n",
    "# Filter st_df based on nameList2 (clean ROW)\n",
    "st_df1= st_df1[st_df.sub_region_1.isin(nameList2)]\n",
    "# print(st_df1)\n",
    "# print(st_df1.shape)\n",
    "\n",
    "#Filter columns so that every column have at least sp_num% of non-zero entries  (clean COLUMN)\n",
    "sp_num = 0.24  #optimized ratio without tremendous loss of dataset\n",
    "st_df2 = st_df1.dropna(thresh=sp_num*len(st_df), axis=1)\n",
    "# print(\"after.........\" )\n",
    "# print(st_df2)\n",
    "# print(st_df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpData = hp_df3[\"hospitalized_new\"]\n",
    "hpData = pd.Series(hpData)\n",
    "\n",
    "st_df2['hospitalized_new'] = hpData.values # Merging the data_set\n",
    "print(st_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert merged dataset into a numpy array\n",
    "myarray = pd.DataFrame(st_df2).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Visualize and Cluster the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the evolution of the search frequency of popular symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values by 0s\n",
    "myarray[pd.isnull(myarray)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = myarray[:,0]\n",
    "time = myarray[:,2]\n",
    "features = myarray[:,3:-1].astype(float)\n",
    "label = myarray[:,-1].astype(int)\n",
    "\n",
    "nData, nFeat = features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of symptom names\n",
    "nameSymptoms = np.array([])\n",
    "for name in st_df2.columns.values[3:-1]:\n",
    "    nameSymptoms = np.append(nameSymptoms, name.lstrip('symptom:'))\n",
    "\n",
    "# print(nameSymptoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most popular searches (rank by # of instances)\n",
    "mostPop = 5\n",
    "\n",
    "arrInst = np.count_nonzero(features, axis=0)\n",
    "# Get indices of most popular searches\n",
    "topSearchInd = np.sort(np.argpartition(arrInst, -mostPop)[-mostPop:])\n",
    "# print(topSearchInd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of symptom names of most popular searches\n",
    "namePopSymptoms = nameSymptoms[topSearchInd]\n",
    "# print(namePopSymptoms)\n",
    "\n",
    "# Get array of features of only the most popular searches\n",
    "popFeatures = features[:,topSearchInd]\n",
    "# print(popFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape search data 2D array into 3D [region, time, feature]\n",
    "\n",
    "uniqueRegions, ctRegions = np.unique(regions, return_counts=True)\n",
    "# print(dict(zip(uniqueRegions, ctRegions)))\n",
    "\n",
    "uniqueTime, ctTime = np.unique(time, return_counts=True)\n",
    "# print(dict(zip(uniqueTime, ctTime)))\n",
    "\n",
    "nRegions = ctTime[0]\n",
    "nTime = ctRegions[0]\n",
    "\n",
    "sdArr = popFeatures.reshape(-1, nTime, mostPop)\n",
    "# print(sdArr.shape)\n",
    "# print(sdArr[0,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap\n",
    "for i in range(mostPop):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    im = ax.imshow(sdArr[:,:,i], cmap='magma', vmin=0, vmax=100)\n",
    "    plt.colorbar(im)\n",
    "\n",
    "    ax.set_xticks(np.arange(nTime))\n",
    "    ax.set_yticks(np.arange(nRegions))\n",
    "\n",
    "    ax.set_xticklabels(uniqueTime)\n",
    "    ax.set_yticklabels(uniqueRegions)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "\n",
    "    for j in range(nTime):\n",
    "        for k in range(nRegions):\n",
    "            text = ax.text(j, k, sdArr[k, j, i], ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax.set_title('Evolution of ' + namePopSymptoms[i])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PCA to reduce data dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the features\n",
    "pca = PCA()\n",
    "pca.fit(features)\n",
    "pcaEVR = pca.explained_variance_ratio_\n",
    "totalPC = len(pcaEVR)\n",
    "cumulativeVar = 100*np.cumsum(pcaEVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshCVar = 95\n",
    "\n",
    "nPC = np.where(cumulativeVar > threshCVar)[0][0]\n",
    "cVar = cumulativeVar[nPC]\n",
    "nPC += 1\n",
    "\n",
    "# print(cVar, nPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cumulative variance vs # of principal components to choose #PCs\n",
    "fig = plt.figure()\n",
    "plt.plot(np.linspace(1, totalPC, totalPC), threshCVar*np.ones((totalPC,)), 'c--')\n",
    "plt.plot(np.linspace(1, totalPC, totalPC), cumulativeVar)\n",
    "plt.plot(nPC, cVar, 'o')\n",
    "\n",
    "plt.text(nPC-3, cVar-4, 'Optimal #PCs: ' + str(nPC))\n",
    "plt.text(totalPC, threshCVar, 'Cumulative Variance Threshold:' + str(threshCVar) + '%', horizontalalignment='right')\n",
    "\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"% Variance Explained\")\n",
    "plt.title(\"Cumulative Variance Explained vs Number of Principal Components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have chosen the #PCs -> now reduce the search trends dataset to this dimensionality\n",
    "pcaRed = PCA(n_components=nPC)\n",
    "pcaRed.fit(features)\n",
    "reducedFeat = pcaRed.transform(features)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(reducedFeat[:,0], reducedFeat[:,1])\n",
    "plt.xlabel(\"PC #1\")\n",
    "plt.ylabel(\"PC #2\")\n",
    "plt.title(\"Search Trends Dataset Reduced to \" + str(nPC) + \"D, Visualizing in 2D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using k-means clustering to evaluate groups in search trends data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nClusters = 6    # To be tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering on PCA-reduced data\n",
    "kmeansRed = KMeans(n_clusters=nClusters, random_state=0)\n",
    "kmeansRed.fit(reducedFeat)\n",
    "y_PredRed = kmeansRed.predict(reducedFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering on original (non-reduced) data\n",
    "kmeansOri = KMeans(n_clusters=nClusters, random_state=0)\n",
    "kmeansOri.fit(features)\n",
    "y_PredOri = kmeansOri.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clustering results for both reduced and non-reduced data\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.scatter(reducedFeat[:,0], reducedFeat[:,1], c=y_PredRed, cmap=plt.cm.get_cmap('viridis', nClusters))\n",
    "plt.colorbar(ticks=range(nClusters))\n",
    "plt.xlabel(\"PC #1\")\n",
    "plt.ylabel(\"PC #2\")\n",
    "plt.title(\"Cluster Labels for \" + str(nPC) + \"D K-Means with \" + str(nClusters) + \" Clusters\")\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.scatter(reducedFeat[:,0], reducedFeat[:,1], c=y_PredOri, cmap=plt.cm.get_cmap('viridis', nClusters))\n",
    "plt.colorbar(ticks=range(nClusters))\n",
    "plt.xlabel(\"PC #1\")\n",
    "plt.ylabel(\"PC #2\")\n",
    "plt.title(\"Cluster Labels for \" + str(totalPC) + \"D K-Means (Unreduced Data) with \" + str(nClusters) + \" Clusters\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN0fg+0w8Z9nRsJs3Ucih0p",
   "include_colab_link": true,
   "name": "COMP551_Project_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
